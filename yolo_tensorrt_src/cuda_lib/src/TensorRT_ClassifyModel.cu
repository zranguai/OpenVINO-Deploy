#include "TensorRT_ClassifyModel.hpp"
namespace Classify {

    //using namespace std;

#define GPU_BLOCK_THREADS 512
#define checkRuntime(call)                                                                 \
  do {                                                                                     \
    auto ___call__ret_code__ = (call);                                                     \
    if (___call__ret_code__ != cudaSuccess) {                                              \
      INFO("CUDA Runtime errorğŸ’¥ %s # %s, code = %s [ %d ]", #call,                         \
           cudaGetErrorString(___call__ret_code__), cudaGetErrorName(___call__ret_code__), \
           ___call__ret_code__);                                                           \
      abort();                                                                             \
    }                                                                                      \
  } while (0)

#define checkKernel(...)                 \
  do {                                   \
    { (__VA_ARGS__); }                   \
    checkRuntime(cudaPeekAtLastError()); \
  } while (0)

    enum class NormType : int { None = 0, MeanStd = 1, AlphaBeta = 2 };

    enum class ChannelType : int { None = 0, SwapRB = 1 };

    /* å½’ä¸€åŒ–æ“ä½œï¼Œå¯ä»¥æ”¯æŒå‡å€¼æ ‡å‡†å·®ï¼Œalpha betaï¼Œå’Œswap RB */
    struct Norm {
        float mean[3];
        float std[3];
        float alpha, beta;
        NormType type = NormType::None;
        ChannelType channel_type = ChannelType::None;

        // out = (x * alpha - mean) / std
        static Norm mean_std(const float mean[3], const float std[3], float alpha = 1 / 255.0f,
            ChannelType channel_type = ChannelType::None);

        // out = x * alpha + beta
        static Norm alpha_beta(float alpha, float beta = 0, ChannelType channel_type = ChannelType::None);

        // None
        static Norm None();
    };

    Norm Norm::mean_std(const float mean[3], const float std[3], float alpha,
        ChannelType channel_type) {
        Norm out;
        out.type = NormType::MeanStd;
        out.alpha = alpha;
        out.channel_type = channel_type;
        memcpy(out.mean, mean, sizeof(out.mean));
        memcpy(out.std, std, sizeof(out.std));
        return out;
    }

    Norm Norm::alpha_beta(float alpha, float beta, ChannelType channel_type) {
        Norm out;
        out.type = NormType::AlphaBeta;
        out.alpha = alpha;
        out.beta = beta;
        out.channel_type = channel_type;
        return out;
    }

    Norm Norm::None() { return Norm(); }

    const int NUM_BOX_ELEMENT = 8;  // left, top, right, bottom, confidence, class,
                                    // keepflag, row_index(output)
    const int MAX_IMAGE_BOXES = 1024;
    inline int upbound(int n, int align = 32) { return (n + align - 1) / align * align; }
    static __host__ __device__ void affine_project(float* matrix, float x, float y, float* ox,
        float* oy) {
        *ox = matrix[0] * x + matrix[1] * y + matrix[2];
        *oy = matrix[3] * x + matrix[4] * y + matrix[5];
    }

    

    static dim3 grid_dims(int numJobs) {
        int numBlockThreads = numJobs < GPU_BLOCK_THREADS ? numJobs : GPU_BLOCK_THREADS;
        return dim3(((numJobs + numBlockThreads - 1) / (float)numBlockThreads));
    }

    static dim3 block_dims(int numJobs) {
        return numJobs < GPU_BLOCK_THREADS ? numJobs : GPU_BLOCK_THREADS;
    }

    

    static __global__ void warp_affine_bilinear_and_normalize_plane_kernel(
        uint8_t* src, int src_line_size, int src_width, int src_height, float* dst, int dst_width,
        int dst_height, uint8_t const_value_st, float* warp_affine_matrix_2_3, Norm norm) {
        int dx = blockDim.x * blockIdx.x + threadIdx.x;
        int dy = blockDim.y * blockIdx.y + threadIdx.y;
        if (dx >= dst_width || dy >= dst_height) return;

        float m_x1 = warp_affine_matrix_2_3[0];
        float m_y1 = warp_affine_matrix_2_3[1];
        float m_z1 = warp_affine_matrix_2_3[2];
        float m_x2 = warp_affine_matrix_2_3[3];
        float m_y2 = warp_affine_matrix_2_3[4];
        float m_z2 = warp_affine_matrix_2_3[5];

        float src_x = m_x1 * dx + m_y1 * dy + m_z1;
        float src_y = m_x2 * dx + m_y2 * dy + m_z2;
        float c0, c1, c2;

        if (src_x <= -1 || src_x >= src_width || src_y <= -1 || src_y >= src_height) {
            // out of range
            c0 = const_value_st;
            c1 = const_value_st;
            c2 = const_value_st;
        }
        else {  // åŒçº¿æ€§æ’å€¼è®¡ç®—
            int y_low = floorf(src_y);
            int x_low = floorf(src_x);
            int y_high = y_low + 1;
            int x_high = x_low + 1;
            uint8_t const_value[] = { const_value_st, const_value_st, const_value_st };
            float ly = src_y - y_low;
            float lx = src_x - x_low;
            float hy = 1 - ly;
            float hx = 1 - lx;
            float w1 = hy * hx, w2 = hy * lx, w3 = ly * hx, w4 = ly * lx;
            uint8_t* v1 = const_value;
            uint8_t* v2 = const_value;
            uint8_t* v3 = const_value;
            uint8_t* v4 = const_value;
            if (y_low >= 0) {
                if (x_low >= 0) v1 = src + y_low * src_line_size + x_low * 3;
                if (x_high < src_width) v2 = src + y_low * src_line_size + x_high * 3;
            }
            if (y_high < src_height) {
                if (x_low >= 0) v3 = src + y_high * src_line_size + x_low * 3;
                if (x_high < src_width) v4 = src + y_high * src_line_size + x_high * 3;
            }
            // same to opencv
            c0 = floorf(w1 * v1[0] + w2 * v2[0] + w3 * v3[0] + w4 * v4[0] + 0.5f);
            c1 = floorf(w1 * v1[1] + w2 * v2[1] + w3 * v3[1] + w4 * v4[1] + 0.5f);
            c2 = floorf(w1 * v1[2] + w2 * v2[2] + w3 * v3[2] + w4 * v4[2] + 0.5f);
        }

        if (norm.channel_type == ChannelType::SwapRB) {
            float t = c2;
            c2 = c0;
            c0 = t;
        }

        if (norm.type == NormType::MeanStd) {
            c0 = (c0 * norm.alpha - norm.mean[0]) / norm.std[0];
            c1 = (c1 * norm.alpha - norm.mean[1]) / norm.std[1];
            c2 = (c2 * norm.alpha - norm.mean[2]) / norm.std[2];
        }
        else if (norm.type == NormType::AlphaBeta) {
            c0 = c0 * norm.alpha + norm.beta;
            c1 = c1 * norm.alpha + norm.beta;
            c2 = c2 * norm.alpha + norm.beta;
        }

        int area = dst_width * dst_height;
        float* pdst_c0 = dst + dy * dst_width + dx;
        float* pdst_c1 = pdst_c0 + area;
        float* pdst_c2 = pdst_c1 + area;
        *pdst_c0 = c0;
        *pdst_c1 = c1;
        *pdst_c2 = c2;
    }

    // åˆ©ç”¨CUDAå·¥å…·å¤„ç†çš„èƒ½åŠ›ï¼Œå°†è¾“å…¥å›¾åƒè¿›è¡Œä»¿å°„å˜æ¢ã€åŒçº¿æ€§æ’å€¼å¤„ç†ï¼ŒæŠŠç»“æœå½’ä¸€åŒ–åˆ°è¾“å‡ºå›¾åƒä¸­
    static void warp_affine_bilinear_and_normalize_plane(uint8_t* src, int src_line_size, int src_width,
        int src_height, float* dst, int dst_width,
        int dst_height, float* matrix_2_3,
        uint8_t const_value, const Norm& norm,
        cudaStream_t stream) {
        dim3 grid((dst_width + 31) / 32, (dst_height + 31) / 32);  // CUDAä¸­çš„ä¸‰ç»´å›¾å½¢æ”¯æŒç±»å‹ï¼Œç”¨äºå®šä¹‰ç½‘æ ¼å’Œå—çš„ç»´åº¦
        dim3 block(32, 32);  // è®¡ç®—ç½‘æ ¼çš„ç»´åº¦ï¼Œæ ¹æ®è¾“å‡ºå›¾åƒçš„å®½åº¦å’Œé«˜åº¦è®¡ç®—æ‰€éœ€çš„å—æ•°é‡ï¼Œæ¯ä¸ªCUDAå—çš„ç»´åº¦è®¾ç½®ä¸º32x32ï¼Œè¡¨ç¤ºæ¯ä¸ªå—å¤„ç†1024ä¸ªçº¿ç¨‹

        checkKernel(warp_affine_bilinear_and_normalize_plane_kernel << <grid, block, 0, stream >> > (
            src, src_line_size, src_width, src_height, dst, dst_width, dst_height, const_value,
            matrix_2_3, norm));
    }






    struct AffineMatrix {
        float i2d[6];  // image to dst(network), 2x3 matrix
        float d2i[6];  // dst to image, 2x3 matrix

        void compute(const std::tuple<int, int>& from, const std::tuple<int, int>& to) {
            float scale_x = std::get<0>(to) / (float)std::get<0>(from);
            float scale_y = std::get<1>(to) / (float)std::get<1>(from);
            float scale = std::min(scale_x, scale_y);
            i2d[0] = scale;
            i2d[1] = 0;
            i2d[2] = -scale * std::get<0>(from) * 0.5 + std::get<0>(to) * 0.5 + scale * 0.5 - 0.5;
            i2d[3] = 0;
            i2d[4] = scale;
            i2d[5] = -scale * std::get<1>(from) * 0.5 + std::get<1>(to) * 0.5 + scale * 0.5 - 0.5;

            double D = i2d[0] * i2d[4] - i2d[1] * i2d[3];
            D = D != 0. ? double(1.) / D : double(0.);
            double A11 = i2d[4] * D, A22 = i2d[0] * D, A12 = -i2d[1] * D, A21 = -i2d[3] * D;
            double b1 = -A11 * i2d[2] - A12 * i2d[5];
            double b2 = -A21 * i2d[2] - A22 * i2d[5];

            d2i[0] = A11;
            d2i[1] = A12;
            d2i[2] = b1;
            d2i[3] = A21;
            d2i[4] = A22;
            d2i[5] = b2;
        }
    };

    // å®šä¹‰InferImplç±»ï¼Œç»§æ‰¿Inferæ¥å£
    class InferImpl : public Infer {
    public:
        std::shared_ptr<trt::Infer> trt_;  // æŒ‡å‘TensorRTæ¨ç†å¼•æ“çš„æ™ºèƒ½æŒ‡é’ˆ
        std::string engine_file_;
        int class_num;
        std::vector<std::shared_ptr<trt::Memory<unsigned char>>> preprocess_buffers_;  // ç”¨äºæ¨ç†ä¹‹å‰é¢„å¤„ç†å›¾åƒçš„ç¼“å†²åŒº
        trt::Memory<float> input_buffer_, prob_predict_, output_probarray_;  // è¾“å…¥æ•°æ®å’Œé¢„æµ‹çš„ç¼“å†²åŒº
        int network_input_width_, network_input_height_;  // æ¨¡å‹è¾“å…¥çš„ç»´åº¦
        Norm normalize_;  // ç”¨äºè§„èŒƒåŒ–è¾“å…¥å›¾åƒçš„å¯¹è±¡
        std::vector<int> class_dim;  // è¾“å‡ºç±»åˆ«çš„ç»´åº¦
        bool isdynamic_model_ = false;  // æŒ‡ç¤ºæ¨¡å‹æ˜¯å¦å¯ä»¥å¤„ç†åŠ¨æ€æ‰¹é‡å¤§å°çš„æ ‡å¿—


        virtual ~InferImpl() = default;  // é»˜è®¤ææ„å‡½æ•°ï¼›ç”±äºæ™ºèƒ½æŒ‡é’ˆï¼ŒTensorRT å¯¹è±¡å°†è¢«è‡ªåŠ¨æ¸…ç†

        // æ ¹æ®ç»™å®šçš„æ‰¹æ¬¡å¤§å°ä¸ºè¾“å…¥å’Œè¾“å‡ºç¼“å†²åŒºåˆ†é…å†…å­˜,
        // å®ƒæ£€æŸ¥çš„å¤§å°æ˜¯å¦preprocess_buffers_å°äºbatch_sizeï¼Œå¦‚æœæ˜¯ï¼Œåˆ™å‘Memoryå¯¹è±¡æ·»åŠ æ–°çš„å…±äº«æŒ‡é’ˆ
        void adjust_memory(int batch_size) {
            // the inference batch_size
            size_t input_numel = network_input_width_ * network_input_height_ * 3;
        


            input_buffer_.gpu(batch_size * input_numel);
            prob_predict_.gpu(batch_size * class_num);
            prob_predict_.cpu(batch_size * class_num);
            //output_probarray_.gpu(batch_size * (32 + 1));
            //output_probarray_.cpu(batch_size * (32 + 1));
            
            std::cout << batch_size * class_num * 3 << std::endl;
            if ((int)preprocess_buffers_.size() < batch_size) {
                for (int i = preprocess_buffers_.size(); i < batch_size; ++i)
                    preprocess_buffers_.push_back(std::make_shared<trt::Memory<unsigned char>>());
            }
        }

        // å‡†å¤‡å›¾åƒä»¥è¾“å…¥åˆ°æ¨¡å‹ä¸­
        // è®¡ç®—ä»¿å°„å˜æ¢ä»¥è°ƒæ•´å›¾åƒå¤§å°å¹¶å¯¹å…¶è¿›è¡Œæ ‡å‡†åŒ–ï¼Œç„¶åå°†å›¾åƒæ•°æ®å¤åˆ¶åˆ°GPUå†…å­˜
        // è°ƒç”¨warp_affine_bilinear_and_normalize_planeï¼Œå®ƒå¯èƒ½å¤„ç†å›¾åƒä»¥ä¾¿ä¸ºæ¨¡å‹åšå‡†å¤‡
        void preprocess(int ibatch, const Image& image,
            std::shared_ptr<trt::Memory<unsigned char>> preprocess_buffer, AffineMatrix& affine,
            void* stream = nullptr) {
            affine.compute(std::make_tuple(image.width, image.height),  // å›¾åƒè®¡ç®—çš„ä»¿å°„å˜æ¢ï¼Œå‡†å¤‡è°ƒæ•´å¤§å°
                std::make_tuple(network_input_width_, network_input_height_));  // ç›®æ ‡å°ºå¯¸ä¸ºç½‘ç»œè¾“å…¥çš„å®½åº¦å’Œé«˜åº¦

            // ä¸Šè¿°computeé€šè¿‡è¾“å…¥å›¾åƒå°ºå¯¸å’Œæ¨¡å‹è¾“å…¥å°ºå¯¸ï¼Œå¾—åˆ°æ˜ å°„å…³ç³»

            size_t input_numel = network_input_width_ * network_input_height_ * 3;
            float* input_device = input_buffer_.gpu() + ibatch * input_numel;  // è®¡ç®—è¾“å…¥çš„å…ƒç´ æ•°é‡ï¼Œå¹¶è·å–å½“å‰æ‰¹æ¬¡çš„è¾“å…¥è®¾å¤‡æŒ‡é’ˆ
            size_t size_image = image.width * image.height * 3;  // å›¾åƒè®¡ç®—çš„æ€»å­—èŠ‚æ•°
            size_t size_matrix = upbound(sizeof(affine.d2i), 32);  // è®¡ç®—ä»¿å°„çŸ©é˜µçš„å¤§å°ï¼Œå¯èƒ½è¦å¯¹é½åˆ°32å­—èŠ‚
            uint8_t* gpu_workspace = preprocess_buffer->gpu(size_matrix + size_image);  // åœ¨GPUä¸Šä¸ºå›¾åƒå’Œåˆ†é…çŸ©é˜µå·¥ä½œç©ºé—´
            // è·å–è®¾å¤‡æŒ‡é’ˆï¼Œåˆ†åˆ«æŒ‡å‘ä»¿å°„çŸ©é˜µå’Œå›¾åƒæ•°æ®
            float* affine_matrix_device = (float*)gpu_workspace;  
            uint8_t* image_device = gpu_workspace + size_matrix;  // è·å–è®¾å¤‡æŒ‡é’ˆï¼Œåˆ†åˆ«æŒ‡å‘ä»¿å°„çŸ©é˜µå’Œå›¾åƒæ•°æ®

            uint8_t* cpu_workspace = preprocess_buffer->cpu(size_matrix + size_image);  // åœ¨CPUä¸Šä¸ºå›¾åƒå’Œåˆ†é…çŸ©é˜µå·¥ä½œç©ºé—´
            // è·å–CPUæŒ‡é’ˆï¼Œåˆ†åˆ«æŒ‡å‘ä»¿å°„çŸ©é˜µå’Œå›¾åƒæ•°æ®
            float* affine_matrix_host = (float*)cpu_workspace;
            uint8_t* image_host = cpu_workspace + size_matrix;

            // speed up
            // å°†è¾“å…¥çš„æµæŒ‡é’ˆè½¬æ¢ä¸ºCUDAæµ
            cudaStream_t stream_ = (cudaStream_t)stream;
            // å°†å›¾åƒå’Œä»¿å°„çŸ©é˜µæ•°æ®ä»ä¸»æœºå¤åˆ¶åˆ°CPUå·¥ä½œç©ºé—´
            memcpy(image_host, image.bgrptr, size_image);
            memcpy(affine_matrix_host, affine.d2i, sizeof(affine.d2i));
            // å¼‚æ­¥å°†å›¾åƒæ•°æ®ä»CPUå¤åˆ¶åˆ°GPU
            checkRuntime(
                cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, stream_));
            // å¹³è¡Œå°†ä»¿å°„çŸ©é˜µæ•°æ®ä»CPUå¤åˆ¶åˆ°GPU
            checkRuntime(cudaMemcpyAsync(affine_matrix_device, affine_matrix_host, sizeof(affine.d2i),
                cudaMemcpyHostToDevice, stream_));
            // è°ƒç”¨å‡½æ•°è¿›è¡Œä»¿å°„å˜æ¢å’Œå½’ä¸€åŒ–ï¼Œå°†å¤„ç†åçš„å›¾åƒå­˜å‚¨åˆ°è¾“å…¥è®¾å¤‡æŒ‡é’ˆ
            warp_affine_bilinear_and_normalize_plane(image_device, image.width * 3, image.width,
                image.height, input_device, network_input_width_,
                network_input_height_, affine_matrix_device, 114,
                normalize_, stream_);
        }


        // ä»å†…å­˜ç¼“å†²åŒºåŠ è½½æ¨¡å‹å¹¶è®¾ç½®å„ç§å‚æ•°ï¼ŒåŒ…æ‹¬è¾“å…¥ç»´åº¦å’Œè§„èŒƒåŒ–
        // æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½æˆåŠŸ
        bool load_from_memory(std::vector<uint8_t>& model_data, int class_num) {
            trt_ = trt::load_from_memory(model_data);
            if (trt_ == nullptr) return false;

            trt_->print();


            this->class_num = class_num;

            auto input_dim = trt_->static_dims(0);  // è·å–è¾“å…¥çš„ç»´åº¦
            class_dim = trt_->static_dims(1);  // è·å–ç±»åˆ«çš„ç»´åº¦
            network_input_width_ = input_dim[3];
            network_input_height_ = input_dim[2];

           


            isdynamic_model_ = trt_->has_dynamic_dim();  // æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒåŠ¨æ€ç»´åº¦
            normalize_ = Norm::alpha_beta(1 / 255.0f, 0.0f, ChannelType::SwapRB);  // è®¾ç½®å›¾åƒæ ‡å‡†åŒ–çš„å‚æ•°ï¼Œå°†åƒç´ å€¼ç¼©æ”¾åˆ° [0, 1] ä¹‹é—´å¹¶è¿›è¡Œé€šé“è½¬æ¢
            return true;
        }


        // æ¨ç†
        virtual ConfInfoArray forward(const Image& image, void* stream = nullptr) override {
            auto output = forwards({ image }, stream);
            if (output.empty()) return {};
            return output[0];
        }


        // æ¨ç†å‡½æ•°
        virtual std::vector<ConfInfoArray> forwards(const std::vector<Image>& images, void* stream = nullptr) override {
            int num_image = images.size();
            if (num_image == 0) return {};

            // ä»TensorRTå¼•æ“è·å–è¾“å…¥çš„é™æ€ç»´åº¦ï¼Œå¹¶è·å–æ‰¹æ¬¡å¤§å°
            auto input_dims = trt_->static_dims(0);
            int infer_batch_size = input_dims[0];

            if (infer_batch_size != num_image) {
                if (isdynamic_model_) {
                    infer_batch_size = num_image;
                    input_dims[0] = num_image;
                    if (!trt_->set_run_dims(0, input_dims)) return {};
                    
                }
                else {
                    if (infer_batch_size < num_image) {
                        INFO(
                            "When using static shape model, number of images[%d] must be "
                            "less than or equal to the maximum batch[%d].",
                            num_image, infer_batch_size);
                        return {};
                    }
                }
            }
            adjust_memory(infer_batch_size);  // è°ƒæ•´å†…å­˜ï¼Œç¡®ä¿æ¨ç†åˆ†é…æœ‰è¶³å¤Ÿçš„è¾“å…¥å’Œè¾“å‡º
            
            std::vector<AffineMatrix> affine_matrixs(num_image);  // åˆ›å»ºä¸€ä¸ªAffineMatrixå¯¹è±¡çš„ç®¡ç†ï¼Œç”¨äºå­˜å‚¨æ¯å¼ å›¾åƒçš„ä»¿å°„å˜æ¢çŸ©é˜µ
            cudaStream_t stream_ = (cudaStream_t)stream;  // å°†è¾“å…¥çš„æµè½¬æ¢ä¸ºCUDAæµï¼Œä»¥ä¾¿åœ¨åç»­æ“ä½œä¸­ä½¿ç”¨

            // éå†æ‰€æœ‰è¾“å…¥å›¾åƒï¼Œè°ƒç”¨preprocesså‡½æ•°å¤„ç†æ¯å¼ å›¾åƒï¼ŒåŒ…æ‹¬è°ƒæ•´å¤§å°å’Œå½’ä¸€åŒ–
            for (int i = 0; i < num_image; ++i)
                preprocess(i, images[i], preprocess_buffers_[i], affine_matrixs[i], stream);

            // è·å–è¾“å‡ºé¢„æµ‹çš„GPUæŒ‡é’ˆï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå¤„ç†bindings,å°†è¾“å…¥å’Œè¾“å‡ºåæ ‡çš„æŒ‡é’ˆå­˜å‚¨åœ¨å…¶ä¸­ï¼ŒåŠæ—¶ä¼ é€’ç»™TensorRTçš„æ¨ç†å‡½æ•°ã€‚
            float* prob_output_device = prob_predict_.gpu();
            std::vector<void*> bindings{ input_buffer_.gpu(), prob_output_device };

  

            if (!trt_->forward(bindings, stream)) {
                INFO("Failed to tensorRT forward.");
                return {};
            }

            // å¼‚æ­¥å°†é¢„æµ‹ç»“æœä» GPU å¤åˆ¶åˆ° CPU å†…å­˜
            checkRuntime(cudaMemcpyAsync(prob_predict_.cpu(), prob_predict_.gpu(),
                prob_predict_.gpu_bytes(), cudaMemcpyDeviceToHost, stream_));
            checkRuntime(cudaStreamSynchronize(stream_));  // ç¡®è®¤æ‰€æœ‰æ“ä½œå®Œæˆï¼Œç­‰å¾…CUDAæµåŒæ­¥

            std::vector<ConfInfoArray> arrout(num_image);  // åˆ›å»ºä¸€ä¸ªç»“æœæ•°æ®åº“ï¼Œç”¨äºå­˜å‚¨æ¯å¼ å›¾åƒçš„é¢„æµ‹ä¿¡æ¯
            //int imemory = 0;
            for (int ib = 0; ib < num_image; ++ib) {
                float* parray = prob_predict_.cpu() + ib * (this->class_num);  // éå†æ¯å¼ å›¾åƒï¼Œä»CPUé¢„æµ‹ç»“æœä¸­è·å–å½“å‰å›¾åƒçš„æ¦‚ç‡ååé‡

                int predict_label = std::max_element(parray, parray + this->class_num) - parray;  // ç¡®å®šé¢„æµ‹ç±»åˆ«çš„ä¸‹æ ‡

                float sum_prob = 0;
                for (int i = 0; i < this->class_num; i++) {
                    sum_prob += exp(parray[i]);
                }
                //auto predict_name = _class_names[predict_label];

                float confidence = exp(parray[predict_label]) / sum_prob; // è·å¾—é¢„æµ‹å€¼çš„ç½®ä¿¡åº¦
                ConfInfoArray& output = arrout[ib];
                ConfInfo Conf(confidence, predict_label);
                output.emplace_back(Conf);
            }
            return arrout;
        }
    };


    Infer* loadraw_from_memory(std::vector<uint8_t>& model_data, int class_num) {
        InferImpl* impl = new InferImpl();
        if (!impl->load_from_memory(model_data, class_num)) {
            delete impl;
            impl = nullptr;
        }
        return impl;
    }






    std::shared_ptr<Infer> load_from_memory(std::vector<uint8_t>& model_data,int class_num) {
        return std::shared_ptr<InferImpl>(
            (InferImpl*)loadraw_from_memory(model_data, class_num));
    }


    std::tuple<uint8_t, uint8_t, uint8_t> hsv2bgr(float h, float s, float v) {
        const int h_i = static_cast<int>(h * 6);
        const float f = h * 6 - h_i;
        const float p = v * (1 - s);
        const float q = v * (1 - f * s);
        const float t = v * (1 - (1 - f) * s);
        float r, g, b;
        switch (h_i) {
        case 0:
            r = v, g = t, b = p;
            break;
        case 1:
            r = q, g = v, b = p;
            break;
        case 2:
            r = p, g = v, b = t;
            break;
        case 3:
            r = p, g = q, b = v;
            break;
        case 4:
            r = t, g = p, b = v;
            break;
        case 5:
            r = v, g = p, b = q;
            break;
        default:
            r = 1, g = 1, b = 1;
            break;
        }
        return std::make_tuple(static_cast<uint8_t>(b * 255), static_cast<uint8_t>(g * 255),
            static_cast<uint8_t>(r * 255));
    }

    std::tuple<uint8_t, uint8_t, uint8_t> random_color(int id) {
        float h_plane = ((((unsigned int)id << 2) ^ 0x937151) % 100) / 100.0f;
        float s_plane = ((((unsigned int)id << 3) ^ 0x315793) % 100) / 100.0f;
        return hsv2bgr(h_plane, s_plane, 1);
    }

};